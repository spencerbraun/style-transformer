# Model Date: 3/5/20

## Notes:
- First running model
- Base settings: learning rate 0.0001 (for both models)
- Ran for 2000 iterations
- No model logs due to Colab disconnect

## Output Notes:
- The output is more coherent in shorter sentences
- "What the Dickens" effect again -- latching onto names from Dickensian work as a proxy for sophistication, though not as severe as the model with learning rate 0.001 that was stopped early at 500 iterations
- Low --> high style appears to be more successful than high --> low style